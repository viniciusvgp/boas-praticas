# -*- coding: utf-8 -*-
# -*- mode: org -*-

#+STARTUP: overview indent
#+LANGUAGE: pt_BR
#+OPTIONS:   toc:nil
#+TAGS: noexport(n) deprecated(d) ignore(i)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

#+TITLE:     Realização de Experimentos Computacionais
#+AUTHOR:    Lucas Mello Schnorr, Vinícius Garcia Pinto
#+EMAIL:     {schnorr, vgpinto}@inf.ufrgs.br

# Reserva de nós (SLURM)
# Coleta de dados (bash)

* #2.0 Passos iniciais

Execute os tutoriais precedentes nesta série:
- [[./0_Programa_Teste.org][#0 (Programa Teste)]]
- [[./1_Ferramentas.org][#1 (Ferramentas)]]

Lembre-se
1. Todos os comandos devem ser executados no diretório SEUNOME
2. Existem dois tipos de tarefas
   - *Local*: pode ser feito na máquina do laboratório
   - *Remota*: deve ser feita no parque computacional

* #2.1 Local: Definição do Projeto Experimental

Vamos utilizar o pacote ~DoE.base~ ([[https://cran.r-project.org/web/packages/DoE.base/][descrição mais detalhada]]) da
linguagem ~R~ para gerar um projeto experimental fatorial completo
combinando os fatores a serem analisados.  Como o objetivo nesta série
de tutorial é analisar o tempo de execução e speedup em função da
quantidade de processos e dois tamanhos de problema, definimos dois
fatores: /processes/ e /size/. Testaremos três quantidades de processos:
2, 10 e 20, e dois tamanhos 300 e 400, para um total de 6
combinações possíveis, com três replicações cada.

O comando =set.seed(0)= (veja abaixo) é utilizado para fixar uma ordem
aleatória, permitindo a geração do mesmo ordenamento da bateria
experimental. O código abaixo pode ser executado no interpretador =R=
com o pacote =DoE.base= instalado.

#+begin_src R :results output :exports both :session *R* :eval no-export
library(DoE.base)
set.seed(0)
btmz_erad <- fac.design(factor.names = list(
                                size = c(300, 400),
                                processes = c(2, 10, 20)),
               replications=3, 
               randomize=TRUE)
print(btmz_erad)
#+end_src

#+RESULTS:
#+begin_example

creating full factorial with 6 runs ...

   run.no run.no.std.rp size processes Blocks
1       1           6.1  400        20     .1
2       2           2.1  400         2     .1
3       3           5.1  300        20     .1
4       4           4.1  400        10     .1
5       5           3.1  300        10     .1
6       6           1.1  300         2     .1
7       7           6.2  400        20     .2
8       8           5.2  300        20     .2
9       9           3.2  300        10     .2
10     10           2.2  400         2     .2
11     11           1.2  300         2     .2
12     12           4.2  400        10     .2
13     13           2.3  400         2     .3
14     14           4.3  400        10     .3
15     15           6.3  400        20     .3
16     16           3.3  300        10     .3
17     17           1.3  300         2     .3
18     18           5.3  300        20     .3
class=design, type= full factorial 
NOTE: columns run.no and run.no.std.rp  are annotation, 
 not part of the data frame
#+end_example

A função ~fac.design~ gerou uma sequência aleatória de combinações
que serão executadas para que possamos avaliar a influência de cada
fator no desempenho da aplicação. Exportaremos o projeto gerado para
um arquivo ~CSV~ que deverá ser registrado juntamente com os /logs/
contendo os resultados brutos do experimento. 

#+begin_src R :results output :exports both :session *R* :eval no-export
export.design(btmz_erad,
              filename = "projeto-experimental",
              type = "csv",
              replace = TRUE
              )
#+end_src

#+RESULTS:

Vejamos o conteúdo do arquivo CSV criado:

#+begin_src shell :results output :exports both :eval no-export
cat projeto-experimental.csv
#+end_src

#+RESULTS:
#+begin_example
"name","run.no.in.std.order","run.no","run.no.std.rp","size","processes","Blocks"
"1","6",1,"6.1","400","20",".1"
"2","2",2,"2.1","400","2",".1"
"3","5",3,"5.1","300","20",".1"
"4","4",4,"4.1","400","10",".1"
"5","3",5,"3.1","300","10",".1"
"6","1",6,"1.1","300","2",".1"
"7","6",7,"6.2","400","20",".2"
"8","5",8,"5.2","300","20",".2"
"9","3",9,"3.2","300","10",".2"
"10","2",10,"2.2","400","2",".2"
"11","1",11,"1.2","300","2",".2"
"12","4",12,"4.2","400","10",".2"
"13","2",13,"2.3","400","2",".3"
"14","4",14,"4.3","400","10",".3"
"15","6",15,"6.3","400","20",".3"
"16","3",16,"3.3","300","10",".3"
"17","1",17,"1.3","300","2",".3"
"18","5",18,"5.3","300","20",".3"
#+end_example

Para facilitar o processo, este arquivo [[./projeto-experimental.csv][está registrado aqui]], e pode
ser baixado no parque computacional da seguinte forma:

#+begin_src shell :results output
cd ~/SEUNOME
wget https://gitlab.com/schnorr/erad19/raw/master/projeto-experimental.csv
#+end_src

* #2.2 Remote: Script de execução do projeto

Acessaremos a plataforma =PCAD= do GPPD/INF-UFRGS para execução de
experimentos de demonstração. Ilustraremos, no exemplo abaixo, um
/script/ para execução não-interativa da aplicação. Para efeitos de
simplificação, foi gerado previamente um projeto experimental (arquivo
~projeto-experimental.csv~). Veja passo #2.1 para gerá-lo.

Vamos revisar o /script/ que automatiza a execução do experimento.

#+begin_src shell :tangle executa-projeto.slurm
#!/bin/bash
#SBATCH --job-name=exp0
#SBATCH --time=01:00:00
#SBATCH --nodes=2
#SBATCH --tasks-per-node=10
#SBATCH --ntasks=20
#SBATCH --partition=tupi

# Diretório de base (atualize para seu caso
export BASE=~/SEUNOME/

# Ingressar no diretório de base
pushd $BASE

# Criar um diretório para conter todos os resultados
rm -rf $SLURM_JOB_NAME
mkdir -p $SLURM_JOB_NAME
pushd $SLURM_JOB_NAME

# Define o Machine File necessário para MPI
MACHINEFILE="nodes.$SLURM_JOB_ID"
srun $SRUN_PACK /bin/hostname | sort -n | awk "{print $2}" > $MACHINEFILE

# Verificar se projeto experimental é fornecido
PROJETO=$BASE/projeto-experimental.csv
if [[ -f $PROJETO ]]; then
  echo "O projeto experimental é o seguinte"
  cat $PROJETO | sed -e "s/^/PROJETO|/"
  # Salva o projeto no diretório corrente (da saída)
  cp $PROJETO .
else
  echo "Arquivo $PROJETO está faltando."
  exit
fi

# Verificar se programa é fornecido
PROGRAMA=$BASE/mpi_mm.c
if [[ -f $PROGRAMA ]]; then
  echo "O programa é o seguinte"
  cat $PROGRAMA | sed -e "s/^/PROGRAMA|/"
  # Salva o programa no diretório corrente (da saída)
  cp $PROGRAMA .
else
  echo "Arquivo $PROGRAMA está faltando."
  exit
fi

# Ler o projeto experimental, e para cada experimento
tail -n +2 $PROJETO |
while IFS=, read -r name runnoinstdorder runno runnostdrp \
	 size processes Blocks
do
    # Limpar valores
    export name=$(echo $name | sed "s/\"//g")
    export processes=$(echo $processes | sed "s/\"//g")
    export size=$(echo $size | sed "s/\"//g")

    # Definir uma chave única
    KEY="$name-$processes-$size"

    # Usar a versão apropriada de MPI
    MPICC=mpicc   #$(spack location -i openmpi@$mpi)/bin/mpicc
    MPIRUN=mpirun #$(spack location -i openmpi@$mpi)/bin/mpirun

    # Altera o código fonte com o tamanho do problema
    sed -i -e "s/#define NRA \(.*\)$/#define NRA $size/" \
           -e "s/#define NCA \(.*\)$/#define NCA $size/" $PROGRAMA

    # Compilar o programa com a versão apropriada
    $MPICC $PROGRAMA -o mpi_mm
    ls -l mpi_mm
    ldd mpi_mm
    sync

    echo $KEY

    # Prepara comando de execução
    runline=""
    runline+="mpirun -np $processes "

    runline+=" -machinefile $MACHINEFILE "
#    runline+="--mca oob_tcp_if_include 192.168.30.0/24 --mca btl_tcp_if_include 192.168.30.0/24 "
    runline+="./mpi_mm "
    runline+="> ${KEY}.log"

    # 3.3 Executar o experimento
    echo "Running >> $runline <<"
    eval "$runline < /dev/null"
    echo "Done!"
done

cp $BASE/slurm-$SLURM_JOB_ID.out .

exit
#+end_src

#+RESULTS:

Para facilitar o processo, este arquivo [[./executa-projeto.slurm][está registrado aqui]], e pode
ser baixado no parque computacional da seguinte forma:

#+begin_src shell :results output
cd ~/SEUNOME
wget https://gitlab.com/schnorr/erad19/raw/master/executa-projeto.slurm
#+end_src

* Introdução ao Slurm                                              :noexport:

Neste tutorial usaremos o gerenciador de filas =Slurm= ([[https://slurm.schedmd.com][site oficial]]). O
~Slurm~ é uma ferramente /open-source/ que permite a execução de /jobs/
interativos ou não-interativos.

O comando ~salloc~ abaixo exemplifica como pode ser realizada
solicitação de um /job/ interativo nomeado =MeuJobErad= na partição de
nome ~hype~, pelo período de 1 hora e 30 minutos.

#+begin_src shell :results output :exports both
salloc -p hype -J MeuJobErad -t 00:10:00
#+end_src

Quando a solicitação for atendida (o que pode ocorrer imediatamente
caso a plataforma esteja ociosa), o usuário estará apto a acessar a
máquina requisitada via ~ssh~ ou a executar diretamente sua aplicação
por meio do comando ~srun~.

Para /jobs/ não-interativos (recomendados, pela automatização possível)
deve-se utilizar o comando ~sbatch~. Neste caso, o usuário fornecerá um
/script/ contendo todos os passos para realizar o experimento na
plataforma desejada. /Jobs/ não-interativos são bastante úteis quando a
plataforma é compartilhada entre muitos usuários estando
frequentemente ocupada e com uma significativa fila de espera. Neste
cenário, o /job/ do usuário poderá executar a qualquer momento após a
submissão, podendo ser iniciado em alguns segundos ou até mesmo após
vários dias da submissão. A política de filas não necessariamente é
/FIFO (First-In First-Out)/ pois alguns usuários podem ter preferência
sobre outros, por exemplo, contas internas /vs/ externas, preferência ao
proprietário/financiador da plataforma, preferência a equipe de
manutenção/suporte, etc.

Um ~job~ termina após uma das seguintes condições (a que occorer
primeiro): a execução da última linha do /script/ fornecido ou após o
término do tempo de processamento solicitado na reserva. Além destes
dois casos bases, o /job/ também pode ser encerrado por pedido do
usuário através do comando ~scancel~ ou por situações inesperadas como
problemas na plataforma ou comandos do administrador. 

O código abaixo ilustra um exemplo de /script/ a ser submetido com o
comando ~sbatch script-exemplo.sh~ ([[./script-exemplo.sh][veja o arquivo aqui]]). Este /script/
exemplo solicita a reserva de 2 nós na plataforma =hype= pelo período
máximo de 40 minutos. As saídas padrão (/stdout/) e de erro (/stdin/)
serão redirecionadas para arquivos nomeados com o identificador do
/job/.

#+begin_src shell :results output :exports both :tangle script-exemplo.sh :eval no-exoort
#!/bin/bash
#SBATCH --nodes=2
#SBATCH --partition=hype
#SBATCH --time=00:20:00
#SBATCH --output=%x_%j.out
#SBATCH --error=%x_%j.err

# Comandos para execução do experimento 

#+end_src

* Execução de uma aplicação paralela exemplo                       :noexport:
** Obtenção e Configuração da Aplicação

Utilizaremos como exemplo uma aplicação da versão /Multi-Zone do
conjunto de /benchmarks/ do /NAS Parallel Benchmarks/ (NPB, [[https://www.nas.nasa.gov/publications/npb.html][mais
informações aqui]]), em específico a aplicação ~BT~ (/Block Tri-diagonal/)
com as classes W (execução local), A e D (execução no =PCAD=).  As
classes nos /benchmarks/ NAS representam diferentes tamanhos de entrada
do problema. A aplicação ~BT~ resolve um sistema sintético de equações
diferenciais parciais não lineares.  A versão /Multi-Zone/ é a variante
do NPB com implementações híbridas combinando MPI e OpenMP. Neste
contexto, escolhemos a aplicação ~BT-MZ~ para este tutorial pois é a que
apresenta maiores desafios quanto ao balanceamento de carga em
comparação com os outros /benchmarks/ da variante /Multi-Zone/.

#+BEGIN_COMMENT Vinícius
- [x] Completar aqui com detalhes do BT em comparação com outros
      benchmarks Explicar o MZ (multi-zone)
#+END_COMMENT

#+BEGIN_COMMENT Lucas
- [x] Demonstrar como fazer para usar um MPI instalado com o spack
#+END_COMMENT

#+begin_src shell :results output :exports both :eval no-export
wget https://www.nas.nasa.gov/assets/npb/NPB3.4-MZ.tar.gz
tar -xf NPB3.4-MZ.tar.gz
cd NPB3.4-MZ/NPB3.4-MZ-MPI
cp config/NAS.samples/make.def.gcc_mpich config/make.def
make bt-mz CLASS=A
make bt-mz CLASS=W
#+end_src

A etapa de compilação dos /benchmarks/ do NAS utiliza arquivos de
configuração, como o ~config/make.def~ dos comandos acima, para indicar
quais compiladores e bibliotecas devem ser empregados na construção
dos binários. Para utilizar compiladores e bibliotecas que não estão
no ~PATH~ padrão, basta editar este arquivo. Tal estratégia pode ser
usada para construção de binários utilizando uma implementação ~MPI~
instalada com o ~Spack~ conforme ilustrado na etapa anterior deste
tutorial. 

** Execução dos Experimentos (manualmente)

Faremos a execução dos experimentos na ordem definida no projeto
experimental. 

#+begin_src shell :results output :exports code :eval no-export
tail -n +2 btmz-exec-order.csv |
while IFS=, read -r name runnoinstdorder runno runnostdrp \
	 threads processes class Blocks
do
    # OpenMP threads
    runline="OMP_NUM_THREADS=$threads "
    # MPI processes
    runline+="mpirun -np $processes "
    # Binary
    runline+="bin/bt-mz.$class.x "
    # Log
    runline+="> btmz-$runno-$threads-$processes-$class.log"
 
    echo "Running >> $runline <<"
    eval "$runline < /dev/null"
    echo "Done!"
done 
#+end_src

** Execução Não-Interativa de Experimentos com /Slurm/ na plataforma =PCAD=
* Controle e Registro

Veja [[./Controle.org][mais informações aqui]] (opcional).

* Local Variables                                                  :noexport:
# Local Variables:
# eval: (ox-extras-activate '(ignore-headlines))
# eval: (setq org-latex-listings t)
# eval: (setq org-latex-packages-alist '(("" "listings")))
# eval: (setq org-latex-packages-alist '(("" "listingsutf8")))
# eval: (setq ispell-local-dictionary "brasileiro")
# eval: (flyspell-mode t)
# End:
